{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "ddc7f24d",
      "metadata": {
        "id": "ddc7f24d"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The code is modified from\n",
        "https://github.com/xiaohu2015/nngen/blob/main/models/diffusion_models,\n",
        "https://github.com/TeaPearce/Conditional_Diffusion_MNIST,\n",
        "https://www.bilibili.com/video/BV1b541197HX/\n",
        "\n",
        "Diffusion model is based on \"CLASSIFIER-FREE DIFFUSION GUIDANCE\" and \"Denoising Diffusion Implicit Models\"\n",
        "https://arxiv.org/abs/2207.12598,\n",
        "https://arxiv.org/abs/2010.02502\n",
        "'''\n",
        "\n",
        "import os\n",
        "import math\n",
        "from abc import abstractmethod\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import autoaugment, RandomErasing\n",
        "from glob import glob\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import ConcatDataset\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "39e38aca",
      "metadata": {
        "id": "39e38aca"
      },
      "outputs": [],
      "source": [
        "# use sinusoidal position embedding to encode time step (https://arxiv.org/abs/1706.03762)\n",
        "def timestep_embedding(timesteps, dim, max_period=10000):\n",
        "    \"\"\"\n",
        "    Create sinusoidal timestep embeddings.\n",
        "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
        "                      These may be fractional.\n",
        "    :param dim: the dimension of the output.\n",
        "    :param max_period: controls the minimum frequency of the embeddings.\n",
        "    :return: an [N x dim] Tensor of positional embeddings.\n",
        "    \"\"\"\n",
        "    half = dim // 2\n",
        "    freqs = torch.exp(\n",
        "        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
        "    ).to(device=timesteps.device)\n",
        "    args = timesteps[:, None].float() * freqs[None]\n",
        "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "    if dim % 2:\n",
        "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "fd7899b8",
      "metadata": {
        "id": "fd7899b8"
      },
      "outputs": [],
      "source": [
        "# define TimestepEmbedSequential to support `time_emb` as extra input\n",
        "class TimestepBlock(nn.Module):\n",
        "    @abstractmethod\n",
        "    def foward(self, x, emb):\n",
        "        \"\"\"\n",
        "        Apply the module to `x` given `emb` timestep embeddings.\n",
        "        \"\"\"\n",
        "\n",
        "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
        "    \"\"\"\n",
        "    A sequential module that passes timestep embeddings to the children that\n",
        "    support it as an extra input.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x, t_emb, c_emb, mask):\n",
        "        for layer in self:\n",
        "            if(isinstance(layer, TimestepBlock)):\n",
        "                x = layer(x, t_emb, c_emb, mask)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "        return x\n",
        "\n",
        "def norm_layer(channels):\n",
        "    return nn.GroupNorm(32, channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "c1545d5c",
      "metadata": {
        "id": "c1545d5c"
      },
      "outputs": [],
      "source": [
        "# Residual block\n",
        "class Residual_block(TimestepBlock):\n",
        "    def __init__(self, in_channels, out_channels, time_channels, class_channels, dropout):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            norm_layer(in_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.time_emb = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_channels, out_channels)\n",
        "        )\n",
        "\n",
        "        self.class_emb = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(class_channels, out_channels)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.time_emb = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_channels, out_channels)\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            norm_layer(out_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self, x, t, c, mask):\n",
        "        \"\"\"\n",
        "        `x` has shape `[batch_size, in_dim, height, width]`\n",
        "        `t` has shape `[batch_size, time_dim]`\n",
        "        `c` has shape `[batch_size, class_dim]`\n",
        "        `mask` has shape `[batch_size, ]`\n",
        "        \"\"\"\n",
        "        h = self.conv1(x)\n",
        "        emb_t = self.time_emb(t)\n",
        "        emb_c = self.class_emb(c)*mask[:, None]\n",
        "        h += (emb_t[:,:, None, None] + emb_c[:,:, None, None])\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        return h + self.shortcut(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "33bd1a09",
      "metadata": {
        "id": "33bd1a09"
      },
      "outputs": [],
      "source": [
        "# Attention block with shortcut\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, channels, num_heads=1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        assert channels % num_heads == 0\n",
        "\n",
        "        self.norm = norm_layer(channels)\n",
        "        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1, bias=False)\n",
        "        self.proj = nn.Conv2d(channels, channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        qkv = self.qkv(self.norm(x))\n",
        "        q, k, v = qkv.reshape(B*self.num_heads, -1, H*W).chunk(3, dim=1)\n",
        "        scale = 1. / math.sqrt(math.sqrt(C // self.num_heads))\n",
        "        attn = torch.einsum(\"bct,bcs->bts\", q * scale, k * scale)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        h = torch.einsum(\"bts,bcs->bct\", attn, v)\n",
        "        h = h.reshape(B, -1, H, W)\n",
        "        h = self.proj(h)\n",
        "        return h + x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "9321c811",
      "metadata": {
        "id": "9321c811"
      },
      "outputs": [],
      "source": [
        "# upsample\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, channels, use_conv):\n",
        "        super().__init__()\n",
        "        self.use_conv = use_conv\n",
        "        if use_conv:\n",
        "            self.conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        if self.use_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "# downsample\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, channels, use_conv):\n",
        "        super().__init__()\n",
        "        self.use_conv = use_conv\n",
        "        if use_conv:\n",
        "            self.op = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
        "        else:\n",
        "            self.op = nn.AvgPool2d(stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.op(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "a7988fdc",
      "metadata": {
        "id": "a7988fdc"
      },
      "outputs": [],
      "source": [
        "class UnetModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=3,\n",
        "                 model_channels=128,\n",
        "                 out_channels=3,\n",
        "                 num_res_blocks=2,\n",
        "                 attention_resolutions=(8,16),\n",
        "                 dropout=0,\n",
        "                 channel_mult=(1,2,2,2),\n",
        "                 conv_resample=True,\n",
        "                 num_heads=4,\n",
        "                 class_num=21\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.model_channels = model_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attention_resolutions = attention_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.channel_mult = channel_mult\n",
        "        self.conv_resample = conv_resample\n",
        "        self.num_heads = num_heads\n",
        "        self.class_num = class_num\n",
        "\n",
        "        #time embedding\n",
        "        time_emb_dim = model_channels*4\n",
        "        self.time_emb = nn.Sequential(\n",
        "                nn.Linear(model_channels, time_emb_dim),\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(time_emb_dim, time_emb_dim)\n",
        "        )\n",
        "\n",
        "        #class embedding\n",
        "        class_emb_dim = model_channels\n",
        "        self.class_emb = nn.Embedding(class_num, class_emb_dim)\n",
        "\n",
        "        #down blocks\n",
        "        self.down_blocks = nn.ModuleList([\n",
        "            TimestepEmbedSequential(nn.Conv2d(in_channels, model_channels, kernel_size=3, padding=1))\n",
        "        ])\n",
        "        down_block_channels = [model_channels]\n",
        "        ch = model_channels\n",
        "        ds = 1\n",
        "        for level, mult in enumerate(channel_mult):\n",
        "            for _ in range(num_res_blocks):\n",
        "                layers = [Residual_block(ch, model_channels*mult, time_emb_dim, class_emb_dim, dropout)]\n",
        "                ch = model_channels*mult\n",
        "                if ds in attention_resolutions:\n",
        "                    layers.append(AttentionBlock(ch, num_heads))\n",
        "                self.down_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                down_block_channels.append(ch)\n",
        "            if level != len(channel_mult)-1: # don't use downsample for the last stage\n",
        "                self.down_blocks.append(TimestepEmbedSequential(Downsample(ch, conv_resample)))\n",
        "                down_block_channels.append(ch)\n",
        "                ds*=2\n",
        "\n",
        "        #middle blocks\n",
        "        self.middle_blocks = TimestepEmbedSequential(\n",
        "            Residual_block(ch, ch, time_emb_dim, class_emb_dim, dropout),\n",
        "            AttentionBlock(ch, num_heads),\n",
        "            Residual_block(ch, ch, time_emb_dim, class_emb_dim, dropout)\n",
        "        )\n",
        "\n",
        "        #up blocks\n",
        "        self.up_blocks = nn.ModuleList([])\n",
        "        for level, mult in enumerate(channel_mult[::-1]):\n",
        "            for i in range(num_res_blocks+1):\n",
        "                layers = [\n",
        "                    Residual_block(ch+down_block_channels.pop(), model_channels*mult,\\\n",
        "                                   time_emb_dim, class_emb_dim, dropout)]\n",
        "                ch = model_channels*mult\n",
        "                if ds in attention_resolutions:\n",
        "                    layers.append(AttentionBlock(ch, num_heads))\n",
        "                if level!=len(channel_mult)-1 and i==num_res_blocks:\n",
        "                    layers.append(Upsample(ch, conv_resample))\n",
        "                    ds //= 2\n",
        "                self.up_blocks.append(TimestepEmbedSequential(*layers))\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            norm_layer(ch),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(ch, out_channels, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, timesteps, c, mask):\n",
        "        \"\"\"\n",
        "        Apply the model to an input batch.\n",
        "        :param x: an [N x C x H x W] Tensor of inputs.\n",
        "        :param timesteps: a 1-D batch of timesteps.\n",
        "        :param c: a 1-D batch of classes.\n",
        "        :param mask: a 1-D batch of conditioned/unconditioned.\n",
        "        :return: an [N x C x ...] Tensor of outputs.\n",
        "        \"\"\"\n",
        "        hs = []\n",
        "        # time step and class embedding\n",
        "        t_emb = self.time_emb(timestep_embedding(timesteps, dim=self.model_channels))\n",
        "        c_emb = self.class_emb(c)\n",
        "\n",
        "\n",
        "        # down stage\n",
        "        h = x\n",
        "        for module in self.down_blocks:\n",
        "            h = module(h, t_emb, c_emb, mask)\n",
        "#             print(h.shape)\n",
        "            hs.append(h)\n",
        "\n",
        "        # middle stage\n",
        "        h = self.middle_blocks(h, t_emb, c_emb, mask)\n",
        "\n",
        "        # up stage\n",
        "        for module in self.up_blocks:\n",
        "#             print(h.shape, hs[-1].shape)\n",
        "            cat_in = torch.cat([h, hs.pop()], dim=1)\n",
        "            h = module(cat_in, t_emb, c_emb, mask)\n",
        "\n",
        "        return self.out(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "58814dac",
      "metadata": {
        "id": "58814dac"
      },
      "outputs": [],
      "source": [
        "# beta schedule\n",
        "def linear_beta_schedule(timesteps):\n",
        "    scale = 1000 / timesteps\n",
        "    beta_start = scale * 0.0001\n",
        "    beta_end = scale * 0.02\n",
        "    return torch.linspace(beta_start, beta_end, timesteps, dtype=torch.float64)\n",
        "\n",
        "def sigmoid_beta_schedule(timesteps):\n",
        "    betas = torch.linspace(-6, 6, timesteps)\n",
        "    betas = torch.sigmoid(betas)/(betas.max()-betas.min())*(0.02-betas.min())/10\n",
        "    return betas\n",
        "\n",
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    \"\"\"\n",
        "    cosine schedule\n",
        "    as proposed in https://arxiv.org/abs/2102.09672\n",
        "    \"\"\"\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps, dtype=torch.float64)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0, 0.999)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "761c989f",
      "metadata": {
        "id": "761c989f"
      },
      "outputs": [],
      "source": [
        "class GaussianDiffusion:\n",
        "    def __init__(\n",
        "        self,\n",
        "        timesteps=1000,\n",
        "        beta_schedule='cosine'\n",
        "    ):\n",
        "        self.timesteps = timesteps\n",
        "\n",
        "        if beta_schedule == 'linear':\n",
        "            betas = linear_beta_schedule(timesteps)\n",
        "        elif beta_schedule == 'cosine':\n",
        "            betas = cosine_beta_schedule(timesteps)\n",
        "        elif beta_schedule == 'sigmoid':\n",
        "            betas = sigmoid_beta_schedule(timesteps)\n",
        "        else:\n",
        "            raise ValueError(f'unknown beta schedule {beta_schedule}')\n",
        "        self.betas = betas\n",
        "\n",
        "        self.alphas = 1. - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
        "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.)\n",
        "\n",
        "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
        "        self.log_one_minus_alphas_cumprod = torch.log(1.0 - self.alphas_cumprod)\n",
        "        self.sqrt_recip_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod)\n",
        "        self.sqrt_recipm1_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod - 1)\n",
        "\n",
        "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
        "        self.posterior_variance = (\n",
        "            self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
        "        )\n",
        "        # below: log calculation clipped because the posterior variance is 0 at the beginning\n",
        "        # of the diffusion chain\n",
        "        #self.posterior_log_variance_clipped = torch.log(self.posterior_variance.clamp(min =1e-20))\n",
        "        self.posterior_log_variance_clipped = torch.log(\n",
        "            torch.cat([self.posterior_variance[1:2], self.posterior_variance[1:]])\n",
        "        )\n",
        "\n",
        "        self.posterior_mean_coef1 = (\n",
        "            self.betas * torch.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
        "        )\n",
        "        self.posterior_mean_coef2 = (\n",
        "            (1.0 - self.alphas_cumprod_prev)\n",
        "            * torch.sqrt(self.alphas)\n",
        "            / (1.0 - self.alphas_cumprod)\n",
        "        )\n",
        "\n",
        "    # get the param of given timestep t\n",
        "    def _extract(self, a, t, x_shape):\n",
        "        batch_size = t.shape[0]\n",
        "        out = a.to(t.device).gather(0, t).float()\n",
        "        out = out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
        "        return out\n",
        "\n",
        "    # forward diffusion (using the nice property): q(x_t | x_0)\n",
        "    def q_sample(self, x_start, t, noise=None):\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x_start)\n",
        "\n",
        "        sqrt_alphas_cumprod_t = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape)\n",
        "        sqrt_one_minus_alphas_cumprod_t = self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
        "\n",
        "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
        "\n",
        "    # Get the mean and variance of q(x_t | x_0).\n",
        "    def q_mean_variance(self, x_start, t):\n",
        "        mean = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
        "        variance = self._extract(1.0 - self.alphas_cumprod, t, x_start.shape)\n",
        "        log_variance = self._extract(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n",
        "        return mean, variance, log_variance\n",
        "\n",
        "    # Compute the mean and variance of the diffusion posterior: q(x_{t-1} | x_t, x_0)\n",
        "    def q_posterior_mean_variance(self, x_start, x_t, t):\n",
        "        posterior_mean = (\n",
        "            self._extract(self.posterior_mean_coef1, t, x_t.shape) * x_start\n",
        "            + self._extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
        "        )\n",
        "        posterior_variance = self._extract(self.posterior_variance, t, x_t.shape)\n",
        "        posterior_log_variance_clipped = self._extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
        "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
        "\n",
        "    # compute x_0 from x_t and pred noise: the reverse of `q_sample`\n",
        "    def predict_start_from_noise(self, x_t, t, noise):\n",
        "        return (\n",
        "            self._extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
        "            self._extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
        "        )\n",
        "\n",
        "    # compute predicted mean and variance of p(x_{t-1} | x_t)\n",
        "    def p_mean_variance(self, model, x_t, t, c, w, clip_denoised=True):\n",
        "        device = next(model.parameters()).device\n",
        "        batch_size = x_t.shape[0]\n",
        "        # predict noise using model\n",
        "        pred_noise_c = model(x_t, t, c, torch.ones(batch_size).int().to(device))\n",
        "        pred_noise_none = model(x_t, t, c, torch.zeros(batch_size).int().to(device))\n",
        "        pred_noise = (1+w)*pred_noise_c - w*pred_noise_none\n",
        "\n",
        "        # get the predicted x_0: different from the algorithm2 in the paper\n",
        "        x_recon = self.predict_start_from_noise(x_t, t, pred_noise)\n",
        "        if clip_denoised:\n",
        "            x_recon = torch.clamp(x_recon, min=-1., max=1.)\n",
        "        model_mean, posterior_variance, posterior_log_variance = \\\n",
        "                    self.q_posterior_mean_variance(x_recon, x_t, t)\n",
        "        return model_mean, posterior_variance, posterior_log_variance\n",
        "\n",
        "    # denoise_step: sample x_{t-1} from x_t and pred_noise\n",
        "    @torch.no_grad()\n",
        "    def p_sample(self, model, x_t, t, c, w, clip_denoised=True):\n",
        "        # predict mean and variance\n",
        "        model_mean, _, model_log_variance = self.p_mean_variance(model, x_t, t,\n",
        "                                                c, w, clip_denoised=clip_denoised)\n",
        "        noise = torch.randn_like(x_t)\n",
        "        # no noise when t == 0\n",
        "        nonzero_mask = ((t != 0).float().view(-1, *([1] * (len(x_t.shape) - 1))))\n",
        "        # compute x_{t-1}\n",
        "        pred_img = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
        "        return pred_img\n",
        "\n",
        "    # denoise: reverse diffusion\n",
        "    @torch.no_grad()\n",
        "    def p_sample_loop(self, model, shape, n_class=21, w=2, mode='random', clip_denoised=True):\n",
        "        batch_size = shape[0]\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "        # generate labels\n",
        "        if mode == 'random':\n",
        "            cur_y = torch.randint(0, n_class, (batch_size,)).to(device)\n",
        "        elif mode == 'all':\n",
        "            if batch_size%n_class!=0:\n",
        "                batch_size = n_class\n",
        "                print('change batch_size to', n_class)\n",
        "            cur_y = torch.tensor([x for x in range(n_class)]*(batch_size//n_class), dtype=torch.long).to(device)\n",
        "        else:\n",
        "            cur_y = torch.ones(batch_size).long().to(device)*int(mode)\n",
        "\n",
        "        # start from pure noise (for each example in the batch)\n",
        "        img = torch.randn(shape, device=device)\n",
        "        imgs = []\n",
        "        for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n",
        "            img = self.p_sample(model, img, torch.full((batch_size,), i, device=device, dtype=torch.long), cur_y, w, clip_denoised)\n",
        "            imgs.append(img.cpu().numpy())\n",
        "        return imgs\n",
        "\n",
        "    # sample new images\n",
        "    @torch.no_grad()\n",
        "    def sample(self, model, image_size, batch_size=8, channels=3, n_class=10, w=2, mode='random', clip_denoised=True):\n",
        "        return self.p_sample_loop(model, (batch_size, channels, image_size, image_size), n_class, w, mode, clip_denoised)\n",
        "\n",
        "    # use ddim to sample\n",
        "    @torch.no_grad()\n",
        "    def ddim_sample(\n",
        "        self,\n",
        "        model,\n",
        "        image_size,\n",
        "        batch_size=8,\n",
        "        channels=3,\n",
        "        ddim_timesteps=50,\n",
        "        n_class = 10,\n",
        "        w = 2,\n",
        "        mode= 'random',\n",
        "        ddim_discr_method=\"uniform\",\n",
        "        ddim_eta=0.0,\n",
        "        clip_denoised=True):\n",
        "        # make ddim timestep sequence\n",
        "        if ddim_discr_method == 'uniform':\n",
        "            c = self.timesteps // ddim_timesteps\n",
        "            ddim_timestep_seq = np.asarray(list(range(0, self.timesteps, c)))\n",
        "        elif ddim_discr_method == 'quad':\n",
        "            ddim_timestep_seq = (\n",
        "                (np.linspace(0, np.sqrt(self.timesteps * .8), ddim_timesteps)) ** 2\n",
        "            ).astype(int)\n",
        "        else:\n",
        "            raise NotImplementedError(f'There is no ddim discretization method called \"{ddim_discr_method}\"')\n",
        "        # add one to get the final alpha values right (the ones from first scale to data during sampling)\n",
        "        ddim_timestep_seq = ddim_timestep_seq + 1\n",
        "        # previous sequence\n",
        "        ddim_timestep_prev_seq = np.append(np.array([0]), ddim_timestep_seq[:-1])\n",
        "\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "        # generate labels\n",
        "        if mode == 'random':\n",
        "            cur_y = torch.randint(0, n_class, (batch_size,)).to(device)\n",
        "        elif mode == 'all':\n",
        "            if batch_size%n_class!=0:\n",
        "                batch_size = n_class\n",
        "                print('change batch_size to', n_class)\n",
        "            cur_y = torch.tensor([x for x in range(n_class)]*(batch_size//n_class), dtype=torch.long).to(device)\n",
        "        else:\n",
        "            cur_y = torch.ones(batch_size).long().to(device)*int(mode)\n",
        "\n",
        "        # start from pure noise (for each example in the batch)\n",
        "        sample_img = torch.randn((batch_size, channels, image_size, image_size), device=device)\n",
        "        seq_img = [sample_img.cpu().numpy()]\n",
        "\n",
        "        for i in tqdm(reversed(range(0, ddim_timesteps)), desc='sampling loop time step', total=ddim_timesteps):\n",
        "            t = torch.full((batch_size,), ddim_timestep_seq[i], device=device, dtype=torch.long)\n",
        "            prev_t = torch.full((batch_size,), ddim_timestep_prev_seq[i], device=device, dtype=torch.long)\n",
        "\n",
        "            # 1. get current and previous alpha_cumprod\n",
        "            alpha_cumprod_t = self._extract(self.alphas_cumprod, t, sample_img.shape)\n",
        "            alpha_cumprod_t_prev = self._extract(self.alphas_cumprod, prev_t, sample_img.shape)\n",
        "\n",
        "            # 2. predict noise using model\n",
        "            pred_noise_c = model(sample_img, t, cur_y, torch.ones(batch_size).int().cuda())\n",
        "            pred_noise_none = model(sample_img, t, cur_y, torch.zeros(batch_size).int().cuda())\n",
        "            pred_noise = (1+w)*pred_noise_c - w*pred_noise_none\n",
        "\n",
        "            # 3. get the predicted x_0\n",
        "            pred_x0 = (sample_img - torch.sqrt((1. - alpha_cumprod_t)) * pred_noise) / torch.sqrt(alpha_cumprod_t)\n",
        "            if clip_denoised:\n",
        "                pred_x0 = torch.clamp(pred_x0, min=-1., max=1.)\n",
        "\n",
        "            # 4. compute variance: \"sigma_t(η)\" -> see formula (16)\n",
        "            # σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)\n",
        "            sigmas_t = ddim_eta * torch.sqrt(\n",
        "                (1 - alpha_cumprod_t_prev) / (1 - alpha_cumprod_t) * (1 - alpha_cumprod_t / alpha_cumprod_t_prev))\n",
        "\n",
        "            # 5. compute \"direction pointing to x_t\" of formula (12)\n",
        "            pred_dir_xt = torch.sqrt(1 - alpha_cumprod_t_prev - sigmas_t**2) * pred_noise\n",
        "\n",
        "            # 6. compute x_{t-1} of formula (12)\n",
        "            x_prev = torch.sqrt(alpha_cumprod_t_prev) * pred_x0 + pred_dir_xt + sigmas_t * torch.randn_like(sample_img)\n",
        "\n",
        "            sample_img = x_prev\n",
        "            if mode == 'all':\n",
        "                seq_img.append(sample_img.cpu().numpy())\n",
        "\n",
        "        if mode == 'all':\n",
        "            return seq_img\n",
        "        else:\n",
        "            return sample_img.cpu().numpy()\n",
        "\n",
        "    # compute train losses\n",
        "    def train_losses(self, model, x_start, t, c, mask_c):\n",
        "        # generate random noise\n",
        "        noise = torch.randn_like(x_start)\n",
        "        # get x_t\n",
        "        x_noisy = self.q_sample(x_start, t, noise=noise)\n",
        "        predicted_noise = model(x_noisy, t, c, mask_c)\n",
        "        loss = F.mse_loss(noise, predicted_noise)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "5aa28aa7",
      "metadata": {
        "id": "5aa28aa7"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "timesteps = 500\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# use MNIST dataset\n",
        "# dataset = datasets.MNIST(root='./dataset/mnist/', train=True, download=True, transform=transform)\n",
        "# train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "class P1Dataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None, dataset_name=None):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.dataset_name = dataset_name\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.df.iloc[idx, 0])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        label = self.df.iloc[idx, 1]\n",
        "\n",
        "        if self.dataset_name == \"svhn\":\n",
        "            label += 10\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "current_dir = os.path.dirname(\"\")\n",
        "parent_dir = os.path.dirname(current_dir)\n",
        "train_dir_mnistm = os.path.join(current_dir, \"hw2_data/digits/mnistm/data\")\n",
        "train_csv_mnistm = os.path.join(current_dir, \"hw2_data/digits/mnistm/train.csv\")\n",
        "train_dir_svhn = os.path.join(current_dir, \"hw2_data/digits/svhn/data\")\n",
        "train_csv_svhn = os.path.join(current_dir, \"hw2_data/digits/svhn/train.csv\")\n",
        "\n",
        "train_dataset_mnistm = P1Dataset(\n",
        "        csv_file=train_csv_mnistm,\n",
        "        img_dir=train_dir_mnistm,\n",
        "        transform=transform,\n",
        "        dataset_name=\"mnistm\",\n",
        "    )\n",
        "train_dataset_svhn = P1Dataset(\n",
        "    csv_file=train_csv_svhn,\n",
        "    img_dir=train_dir_svhn,\n",
        "    transform=transform,\n",
        "    dataset_name=\"svhn\",\n",
        ")\n",
        "\n",
        "train_dataset_combined = ConcatDataset([train_dataset_mnistm, train_dataset_svhn])\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset_combined, batch_size=batch_size, shuffle=True, drop_last=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# define model and diffusion\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = UnetModel(\n",
        "    in_channels=3,\n",
        "    model_channels=96,\n",
        "    out_channels=3,\n",
        "    channel_mult=(1, 2, 2),\n",
        "    attention_resolutions=[],\n",
        "    class_num=21\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "gaussian_diffusion = GaussianDiffusion(timesteps=timesteps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "bf95c29d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "bf95c29d",
        "outputId": "bc2d8388-69d4-477c-8bf1-3a60d9227f41"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x500 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "image = next(iter(train_loader))[0][0].squeeze()\n",
        "label = next(iter(train_loader))[1][0].squeeze()\n",
        "\n",
        "x_start = image\n",
        "\n",
        "gaussian_diffusion = GaussianDiffusion(timesteps=500, beta_schedule='linear')\n",
        "\n",
        "# plt.figure(figsize=(16, 5))\n",
        "# for idx, t in enumerate([0, 100, 300, 400, 499]):\n",
        "#     x_noisy = gaussian_diffusion.q_sample(x_start.to(device), t=torch.tensor([t]).to(device))\n",
        "#     noisy_image = (x_noisy.squeeze() + 1) * 127.5\n",
        "#     if idx==0:\n",
        "#         noisy_image = (x_start.squeeze() + 1) * 127.5\n",
        "#     noisy_image = noisy_image.cpu().numpy().astype(np.uint8)\n",
        "#     plt.subplot(1, 5, 1 + idx)\n",
        "#     plt.imshow(noisy_image)\n",
        "#     plt.axis(\"off\")\n",
        "#     plt.title(f\"t={t}\")\n",
        "# ... existing code ...\n",
        "\n",
        "# Adjust the visualization code\n",
        "plt.figure(figsize=(16, 5))\n",
        "for idx, t in enumerate([0, 100, 300, 400, 499]):\n",
        "    x_noisy = gaussian_diffusion.q_sample(x_start.unsqueeze(0).to(device), t=torch.tensor([t]).to(device))\n",
        "    noisy_image = x_noisy.squeeze(0).permute(1, 2, 0)  # Change from [3, 28, 28] to [28, 28, 3]\n",
        "    if idx == 0:\n",
        "        noisy_image = x_start.permute(1, 2, 0)  # For the original image\n",
        "    noisy_image = (noisy_image + 1) * 127.5\n",
        "    noisy_image = noisy_image.cpu().numpy().astype(np.uint8)\n",
        "    plt.subplot(1, 5, 1 + idx)\n",
        "    plt.imshow(noisy_image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"t={t}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "265b902f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "265b902f",
        "outputId": "bb5cc94e-1b7d-4852-a029-b006d861f9b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-62-0174cec6127e>:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_dict = torch.load(checkpoint_path)\n",
            "Training Combined: 100%|██████████| 1058/1058 [05:01<00:00,  3.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best loss: 0.0257 at epoch 1. Saving model...\n",
            "Epoch:1/40 Training Loss: 0.025746, LR: 0.000058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Combined: 100%|██████████| 1058/1058 [05:01<00:00,  3.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best loss: 0.0243 at epoch 2. Saving model...\n",
            "Epoch:2/40 Training Loss: 0.024301, LR: 0.000003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Combined: 100%|██████████| 1058/1058 [05:01<00:00,  3.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best loss: 0.0231 at epoch 3. Saving model...\n",
            "Epoch:3/40 Training Loss: 0.023078, LR: 0.000028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Combined:  31%|███       | 327/1058 [01:33<03:28,  3.51it/s]"
          ]
        }
      ],
      "source": [
        "# train\n",
        "epochs = 40\n",
        "lr = 1e-4\n",
        "p_uncound = 0.2\n",
        "len_data = len(train_loader)\n",
        "time_end = time.time()\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=epochs,  # Total number of epochs\n",
        "        eta_min=1e-6,  # Minimum learning rate\n",
        "    )\n",
        "\n",
        "checkpoint_path = \"./best_model_ep_1.pt\"\n",
        "\n",
        "model = model.to(device)\n",
        "model_dict = model.state_dict()\n",
        "pretrained_dict = torch.load(checkpoint_path)\n",
        "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "model_dict.update(pretrained_dict)\n",
        "model.load_state_dict(model_dict)\n",
        "\n",
        "model_dir = os.path.join(current_dir, f\"models_ep_{epochs}_bs_{batch_size}_lr_{lr}\")\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "best_loss = float(\"inf\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    progress_bar = tqdm(\n",
        "            enumerate(train_loader),\n",
        "            desc=\"Training Combined\",\n",
        "            total=len(train_loader),\n",
        "        )\n",
        "\n",
        "    for step, (images, labels) in progress_bar:\n",
        "        time_start = time_end\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_size = images.shape[0]\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # random generate mask\n",
        "        z_uncound = torch.rand(batch_size)\n",
        "        batch_mask = (z_uncound>p_uncound).int().to(device)\n",
        "\n",
        "        # sample t uniformally for every example in the batch\n",
        "        t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
        "\n",
        "        loss = gaussian_diffusion.train_losses(model, images, t, labels, batch_mask)\n",
        "\n",
        "        # if step % 100 == 0:\n",
        "        #     time_end = time.time()\n",
        "        #     print(\"Epoch{}/{}\\t  Step{}/{}\\t Loss {:.4f}\\t Time {:.2f}\".format(epoch+1, epochs, step+1, len_data, loss.item(), time_end-time_start))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(\n",
        "                    model_dir, f\"best_model_ep_{epoch+1}_loss_{best_loss:.4f}.pt\"\n",
        "                ),\n",
        "            )\n",
        "            print(f\"New best loss: {best_loss:.4f} at epoch {epoch+1}. Saving model...\")\n",
        "\n",
        "        # Save checkpoint every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        torch.save(\n",
        "            model.state_dict(),\n",
        "            os.path.join(model_dir, f\"model_checkpoint_epoch_{epoch+1}.pt\"),\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        f\"Epoch:{epoch+1}/{epochs} Training Loss: {best_loss:.6f}, LR: {scheduler.get_last_lr()[0]:.6f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a3aec5",
      "metadata": {
        "id": "77a3aec5"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('./saved_models'):\n",
        "    os.mkdir('./saved_models')\n",
        "torch.save(model, './saved_models/Classifier_free_DDIM_MNIST.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c87f5266",
      "metadata": {
        "id": "c87f5266"
      },
      "outputs": [],
      "source": [
        "model = torch.load('./saved_models/Classifier_free_DDIM_MNIST.h5')\n",
        "gaussian_diffusion = GaussianDiffusion(timesteps=500, beta_schedule='linear')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "010d17f5",
      "metadata": {
        "id": "010d17f5"
      },
      "outputs": [],
      "source": [
        "generated_images = gaussian_diffusion.sample(model, 28, batch_size=64, channels=1, n_class=10, w=2, mode='random', clip_denoised=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f393db3",
      "metadata": {
        "id": "3f393db3"
      },
      "outputs": [],
      "source": [
        "# generate new images\n",
        "fig = plt.figure(figsize=(12, 12), constrained_layout=True)\n",
        "gs = fig.add_gridspec(8, 8)\n",
        "\n",
        "imgs = generated_images[-1].reshape(8, 8, 28, 28)\n",
        "for n_row in range(8):\n",
        "    for n_col in range(8):\n",
        "        f_ax = fig.add_subplot(gs[n_row, n_col])\n",
        "        f_ax.imshow((imgs[n_row, n_col]+1.0) * 255 / 2, cmap=\"gray\")\n",
        "        f_ax.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "787e254e",
      "metadata": {
        "id": "787e254e"
      },
      "outputs": [],
      "source": [
        "ddim_generated_images = gaussian_diffusion.ddim_sample(model, 28, batch_size=64, channels=1, ddim_timesteps=50, n_class=10,\n",
        "                                                       w=2, mode='random', ddim_discr_method='quad', ddim_eta=0.0, clip_denoised=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "775baf0d",
      "metadata": {
        "id": "775baf0d"
      },
      "outputs": [],
      "source": [
        "# ddim generate new images\n",
        "fig = plt.figure(figsize=(12, 12), constrained_layout=True)\n",
        "gs = fig.add_gridspec(8, 8)\n",
        "\n",
        "imgs = ddim_generated_images.reshape(8, 8, 28, 28)\n",
        "for n_row in range(8):\n",
        "    for n_col in range(8):\n",
        "        f_ax = fig.add_subplot(gs[n_row, n_col])\n",
        "        f_ax.imshow((imgs[n_row, n_col]+1.0) * 255 / 2, cmap=\"gray\")\n",
        "        f_ax.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b5114a5",
      "metadata": {
        "id": "3b5114a5"
      },
      "outputs": [],
      "source": [
        "gif_generated_images = gaussian_diffusion.ddim_sample(model, 28, batch_size=40, channels=1, ddim_timesteps=100, n_class=10,\n",
        "                                                       w=2, mode='all', ddim_discr_method='quad', ddim_eta=0.0, clip_denoised=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76503d86",
      "metadata": {
        "id": "76503d86"
      },
      "outputs": [],
      "source": [
        "# ddim generate 0 1 2 3 4 5 6 7 8 9\n",
        "fig = plt.figure(figsize=(12, 5), constrained_layout=True)\n",
        "gs = fig.add_gridspec(4, 10)\n",
        "\n",
        "imgs = gif_generated_images[-1].reshape(4, 10, 28, 28)\n",
        "for n_row in range(4):\n",
        "    for n_col in range(10):\n",
        "        f_ax = fig.add_subplot(gs[n_row, n_col])\n",
        "        f_ax.imshow((imgs[n_row, n_col]+1.0) * 255 / 2, cmap=\"gray\")\n",
        "        f_ax.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffe1a39b",
      "metadata": {
        "id": "ffe1a39b"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib auto\n",
        "import imageio\n",
        "from glob import glob\n",
        "\n",
        "# 生成gif\n",
        "def get_imgs(x_seq, imgs_path, h, w, img_size):\n",
        "    if not os.path.exists(imgs_path):\n",
        "        os.mkdir(imgs_path)\n",
        "    for i in tqdm(range(len(x_seq)), desc='generate gif time step', total=len(x_seq)):\n",
        "        fig = plt.figure(figsize=(12, 5), constrained_layout=True)\n",
        "        gs = fig.add_gridspec(h, w)\n",
        "\n",
        "        imgs = x_seq[i].reshape(h, w, img_size, img_size)\n",
        "        for n_row in range(h):\n",
        "            for n_col in range(w):\n",
        "                f_ax = fig.add_subplot(gs[n_row, n_col])\n",
        "                f_ax.imshow((imgs[n_row, n_col]+1.0) * 255 / 2, cmap=\"gray\")\n",
        "                f_ax.axis(\"off\")\n",
        "        plt.savefig('{}/{:04d}.jpg'.format(imgs_path, i), dpi=360)\n",
        "        plt.close()\n",
        "\n",
        "def compose_gif(img_paths, output_path, fps=10):\n",
        "    print(img_paths[:12])\n",
        "    gif_images = []\n",
        "    for path in img_paths:\n",
        "        gif_images.append(imageio.imread(path))\n",
        "    imageio.mimsave(output_path,gif_images,fps=fps)\n",
        "\n",
        "def generate_dif(x_seq, img_path, output_path, fps, h, w, img_size, delete_imgs=True):\n",
        "    print('start generate images')\n",
        "    get_imgs(x_seq, img_path, h, w, img_size)\n",
        "    print('start generate gif')\n",
        "    img_path = img_path + '/*.jpg'\n",
        "    img_ls = sorted(glob(img_path))\n",
        "    compose_gif(img_ls, output_path, fps)\n",
        "    print('start delete images')\n",
        "    if delete_imgs:\n",
        "        for i in img_ls:\n",
        "            os.remove(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f28b0648",
      "metadata": {
        "id": "f28b0648"
      },
      "outputs": [],
      "source": [
        "generate_dif(gif_generated_images, './gif_generate', './gif_generate/generate_mnist.gif', 10, 4, 10, 28)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}